import tweepy
import requests
import os
from datetime import datetime
from dotenv import load_dotenv
import re

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ======================
# åˆå§‹åŒ– API å®¢æˆ·ç«¯
# ======================

# X API Client
client = tweepy.Client(bearer_token=os.getenv("BEARER_TOKEN"))

# Notion Headers
NOTION_HEADERS = {
    "Authorization": f"Bearer {os.getenv('NOTION_API_KEY')}",
    "Content-Type": "application/json",
    "Notion-Version": "2022-06-28"
}

# ======================
# å·¥å…·å‡½æ•°
# ======================

def extract_urls(text):
    return re.findall(r'https?://[^\s]+', text)

def find_first_http_link(text):
    urls = extract_urls(text)
    return urls[0] if urls else None

def is_tool_related(text):
    keywords = [
        'ai', 'äººå·¥æ™ºèƒ½', 'å¤§æ¨¡å‹', 'llm', 'chatgpt', 'å·¥å…·', 'æ•ˆç‡', 'productivity',
        'notion', 'obsidian', 'figma', 'è®¾è®¡', 'å­¦ä¹ ', 'æ•™ç¨‹', 'workflow', 'automation',
        'app', 'è½¯ä»¶', 'å¹³å°', 'å‘å¸ƒ', 'æ›´æ–°', 'æ–°åŠŸèƒ½'
    ]
    text_lower = text.lower()
    return any(k in text_lower for k in keywords)

def classify_tweet(text):
    text_lower = text.lower()
    if any(k in text_lower for k in ['ai', 'äººå·¥æ™ºèƒ½', 'å¤§æ¨¡å‹', 'llm', 'chatgpt']):
        return "AIå·¥å…·"
    elif any(k in text_lower for k in ['notion', 'obsidian', 'æ•ˆç‡', 'productivity', 'workflow']):
        return "æ•ˆç‡å·¥å…·"
    elif any(k in text_lower for k in ['figma', 'è®¾è®¡', 'ui', 'ux']):
        return "è®¾è®¡å·¥å…·"
    elif any(k in text_lower for k in ['å­¦ä¹ ', 'ç¬”è®°', 'çŸ¥è¯†ç®¡ç†', 'readwise']):
        return "å­¦ä¹ æ–¹æ³•"
    else:
        return "å…¶ä»–"

def generate_summary(text, urls=None):
    # ç®€åŒ–ç‰ˆæ‘˜è¦ï¼šå–å‰ 100 å­— + çœç•¥å·
    # è¿›é˜¶ç‰ˆï¼šå¯æ¥å…¥ Qwen/OpenAI API
    if len(text) <= 100:
        return text
    return text[:100] + "..."

def create_notion_page(tweet_data):
    created_time = tweet_data['created_at'].replace('T', ' ').replace('Z', '+00:00')

    data = {
        "parent": { "database_id": os.getenv("NOTION_DATABASE_ID") },
        "properties": {
            "Name": {
                "title": [{ "text": { "content": tweet_data['title'][:100] } }]
            },
            "ç±»å‹": {
                "select": { "name": tweet_data['type'] }
            },
            "æ‘˜è¦": {
                "rich_text": [{ "text": { "content": tweet_data['summary'][:200] } }]
            },
            "åŸæ–‡é“¾æ¥": {
                "url": tweet_data['x_url']
            },
            "å·¥å…·é“¾æ¥": {
                "url": tweet_data.get('tool_url')
            },
            "æ—¶é—´": {
                "date": { "start": created_time }
            },
            "æ¥æº": {
                "rich_text": [{ "text": { "content": tweet_data['author'] } }]
            },
            "å·²è¯»": {
                "checkbox": False
            }
        }
    }

    url = "https://api.notion.com/v1/pages"
    response = requests.post(url, headers=NOTION_HEADERS, json=data)

    if response.status_code == 201:
        print("âœ… æˆåŠŸå†™å…¥ Notionï¼š", tweet_data['title'])
    else:
        print("âŒ å†™å…¥å¤±è´¥ï¼š", response.status_code, response.text)

# ======================
# ä¸»é€»è¾‘
# ======================

def main():
    username = os.getenv("X_USERNAME")
    if not username:
        print("âŒ è¯·åœ¨ .env ä¸­è®¾ç½® X_USERNAME")
        return

    # è·å–ç”¨æˆ· ID
    try:
        user = client.get_user(username=username)
        user_id = user.data.id
    except Exception as e:
        print("âŒ è·å–ç”¨æˆ·å¤±è´¥ï¼š", e)
        return

    # è·å–æœ€è¿‘ 50 æ¡å–œæ¬¢çš„æ¨æ–‡
    try:
        liked_tweets = client.get_liked_tweets(
            id=user_id,
            max_results=50,
            tweet_fields=['created_at', 'author_id', 'entities', 'context_annotations']
        )
    except Exception as e:
        print("âŒ è·å–å–œæ¬¢çš„æ¨æ–‡å¤±è´¥ï¼š", e)
        return

    if not liked_tweets.data:
        print("ğŸ“­ æ²¡æœ‰è·å–åˆ°å–œæ¬¢çš„æ¨æ–‡")
        return

    print(f"ğŸ” æ£€æµ‹åˆ° {len(liked_tweets.data)} æ¡å–œæ¬¢çš„æ¨æ–‡")

    for tweet in liked_tweets.data:
        text = tweet.text
        if not is_tool_related(text):
            continue

        tool_url = find_first_http_link(text)
        title = text.strip().replace('\n', ' ')[:100]
        if len(title) > 97:
            title = title[:97] + "..."

        tweet_data = {
            'title': title,
            'summary': generate_summary(text, [tool_url] if tool_url else []),
            'type': classify_tweet(text),
            'x_url': f"https://x.com/{username}/status/{tweet.id}",
            'tool_url': tool_url,
            'created_at': tweet.created_at.isoformat(),
            'author': f"@{tweet.author_id}"
        }

        create_notion_page(tweet_data)

if __name__ == "__main__":
    main()
